{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84f2e983-31c4-47e1-a965-27560dd80e45",
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import os\n",
    "# 创建参数解析器\n",
    "parser = argparse.ArgumentParser(description=\"Test script to accept dataset path\")\n",
    "parser.add_argument('dataset_dir', type=str, help='Path to the dataset directory')\n",
    "\n",
    "# 解析传入的命令行参数\n",
    "args = parser.parse_args()\n",
    "test_path = args.dataset_dir\n",
    "test_path = test_path.strip()\n",
    "test_path = os.path.join(test_path, \"test2.jsonl\")\n",
    "# 现在可以通过 args.dataset_dir 访问传递的路径参数\n",
    "print(f\"Dataset directory is: {args.dataset_dir}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "250630a0-c9cc-4291-acb8-325a4b8e7f5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.llms.openai.utils import ALL_AVAILABLE_MODELS, CHAT_MODELS\n",
    "from typing import Dict\n",
    "MY_MODELS: Dict[str, int] = {\n",
    "    \"qwen-plus\": 32768,\n",
    "    \"gpt-3.5-turbo\": 4000,\n",
    "    \"moonshot-v1-8k\": 8000,\n",
    "    \"llama3-70b-8192\": 8192,\n",
    "}\n",
    "ALL_AVAILABLE_MODELS.update(MY_MODELS)\n",
    "# 不加入这个字典，会导致它采用Completion而不是Chat Completion接口，Qwen不兼容Completion兼容。\n",
    "CHAT_MODELS.update(MY_MODELS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d5df132-0216-4196-96d6-a6aaacb53b7a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from llama_index.core import VectorStoreIndex, SimpleDirectoryReader, Settings,StorageContext,Settings\n",
    "from llama_index.embeddings.huggingface import HuggingFaceEmbedding\n",
    "from llama_index.llms.huggingface import HuggingFaceLLM\n",
    "from llama_index.core import PromptTemplate\n",
    "from llama_index.llms.ollama import Ollama\n",
    "import torch\n",
    "from llama_index.llms.openai import OpenAI\n",
    "\n",
    "# selected_model = \"/mnt/sda/cl/competition/BDCI/Qwen2.5-7B-Instruct\"\n",
    "\n",
    "\n",
    "# llm = HuggingFaceLLM(context_window=8192,\n",
    "#     max_new_tokens=256,\n",
    "#     generate_kwargs={\"temperature\": 0.0, \"do_sample\": False},\n",
    "#     tokenizer_name=selected_model,\n",
    "#     model_name=selected_model,\n",
    "#     device_map=\"auto\",\n",
    "#     model_kwargs={\"torch_dtype\": torch.float16}                 \n",
    "# )\n",
    "# llm = Ollama(model=\"myllama3.1\", request_timeout=50.0)\n",
    "llm =  OpenAI(\n",
    "    model=\"qwen-plus\",\n",
    "    api_key=\"sk-xxx"\", \n",
    "    api_base=\"https://dashscope.aliyuncs.com/compatible-mode/v1\",\n",
    "    temperature = 0.2\n",
    "\n",
    ")\n",
    "embed_model = HuggingFaceEmbedding(model_name=\"./your_project/BAAI/bge-m3\")\n",
    "# embed_model = HuggingFaceEmbedding(model_name=\"/mnt/sda/cl/competition/BDCI/bce-embedding-base_v1\")\n",
    "\n",
    "Settings.chunk_size = 512                        \n",
    "Settings.embed_model = embed_model\n",
    "# Settings.embed_model = HuggingFaceEmbedding(model_name=\"/mnt/sda/cl/competition/BDCI/bge-large-zh-v1.5\")\n",
    "Settings.llm = llm\n",
    "# index = VectorStoreIndex.from_documents(documents, show_progress=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4405fd8-3a2f-4de0-b774-5fddfafd6109",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core import SimpleDirectoryReader\n",
    "documents = SimpleDirectoryReader(\"./your_project/corpus/userdoc\",recursive=True,filename_as_id=True).load_data()\n",
    "# print(documents[2])\n",
    "# 创建 Markdown 解析器实例\n",
    "# parser = MarkdownNodeParser()\n",
    "# nodes = parser.get_nodes_from_documents(documents,chunk_size=256)\n",
    "# for node in nodes:\n",
    "#     print(node.get_content()+'---------')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a19476da-f97d-4950-b6dd-ee389733967a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from llama_index.core import VectorStoreIndex\n",
    "from llama_index.core.node_parser import SentenceSplitter,SemanticSplitterNodeParser\n",
    "# client = QdrantClient(host=\"localhost\", port=6333)\n",
    "# vector_store = QdrantVectorStore(\n",
    "#     client=client,\n",
    "#     collection_name=\"b\",\n",
    "#     # enable_hybrid=True\n",
    "# )\n",
    "# storage_context = StorageContext.from_defaults(vector_store=vector_store)\n",
    "# index = VectorStoreIndex.from_documents(\n",
    "#     documents,\n",
    "#     storage_context=storage_context,\n",
    "#     chunk_size=256\n",
    "# )\n",
    "# base_splitter  = SentenceSplitter(chunk_size=378,chunk_overlap=128)\n",
    "base_splitter  = SentenceSplitter(chunk_size=512,chunk_overlap=128)\n",
    "\n",
    "# splitter = SemanticSplitterNodeParser(  # 实例化SemanticSplitterNodeParser模型\n",
    "#     buffer_size=1, breakpoint_percentile_threshold=95, embed_model=embed_model\n",
    "# )\n",
    "nodes = base_splitter.get_nodes_from_documents(documents)\n",
    "# for node in nodes:   \n",
    "#     print(node.get_content()+\"--------\")\n",
    "index = VectorStoreIndex(\n",
    "    nodes=nodes,\n",
    "    show_progress=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11a86775-1b88-4a5c-a57d-cf17c6a0650b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import logging\n",
    "import os\n",
    "\n",
    "from typing import Any, Callable, Dict, List, Optional, cast\n",
    "from llama_index.retrievers.bm25 import BM25Retriever\n",
    "from llama_index.core.base.base_retriever import BaseRetriever\n",
    "from llama_index.core.callbacks.base import CallbackManager\n",
    "from llama_index.core.constants import DEFAULT_SIMILARITY_TOP_K\n",
    "from llama_index.core.indices.vector_store.base import VectorStoreIndex\n",
    "from llama_index.core.schema import BaseNode, IndexNode, NodeWithScore, QueryBundle\n",
    "from llama_index.core.storage.docstore.types import BaseDocumentStore\n",
    "from llama_index.core.vector_stores.utils import (\n",
    "    node_to_metadata_dict,\n",
    "    metadata_dict_to_node,\n",
    ")\n",
    "\n",
    "import bm25s\n",
    "import itertools\n",
    "import jieba\n",
    "from nltk.corpus import stopwords\n",
    "stops=set(stopwords.words('chinese'))\n",
    "class ChineseBM25Retriever(BaseRetriever):\n",
    "    \"\"\"A BM25 retriever that uses the BM25 algorithm to retrieve nodes.\n",
    "\n",
    "    Args:\n",
    "        nodes (List[BaseNode], optional):\n",
    "            The nodes to index. If not provided, an existing BM25 object must be passed.\n",
    "        similarity_top_k (int, optional):\n",
    "            The number of results to return. Defaults to DEFAULT_SIMILARITY_TOP_K.\n",
    "        callback_manager (CallbackManager, optional):\n",
    "            The callback manager to use. Defaults to None.\n",
    "        objects (List[IndexNode], optional):\n",
    "            The objects to retrieve. Defaults to None.\n",
    "        object_map (dict, optional):\n",
    "            A map of object IDs to nodes. Defaults to None.\n",
    "        verbose (bool, optional):\n",
    "            Whether to show progress. Defaults to False.\n",
    "    \"\"\"\n",
    "\n",
    "\n",
    "    def _chinese_tokenizer(self, texts: List[str]) -> tuple[str]:\n",
    "        # Use jieba to segment Chinese text\n",
    "        rslts= tuple(itertools.chain.from_iterable(jieba.cut(text) for text in texts))\n",
    "        return rslts\n",
    "    \n",
    "    def __init__(\n",
    "        self,\n",
    "        nodes: Optional[List[BaseNode]] = None,               \n",
    "        similarity_top_k: int = DEFAULT_SIMILARITY_TOP_K,\n",
    "        callback_manager: Optional[CallbackManager] = None,\n",
    "        objects: Optional[List[IndexNode]] = None,\n",
    "        object_map: Optional[dict] = None,\n",
    "        verbose: bool = False,\n",
    "    ) -> None:\n",
    "        \n",
    "        self.similarity_top_k = similarity_top_k\n",
    "\n",
    "\n",
    "        self.stop_words = set(stopwords.words('chinese'))\n",
    "\n",
    "        corpus_tokens = [\n",
    "            [word for word in jieba.cut_for_search(node.get_content()) if word not in self.stop_words]                                  \n",
    "            for node in nodes\n",
    "        ]\n",
    "        self.bm25 = bm25s.BM25()\n",
    "        from llama_index.core.vector_stores.utils import (\n",
    "            node_to_metadata_dict\n",
    "        )\n",
    "        \n",
    "        corpus = [node_to_metadata_dict(node) for node in nodes]\n",
    "        self.bm25.corpus = corpus\n",
    "        self.bm25.index(corpus_tokens, show_progress=True)\n",
    "\n",
    "        super().__init__(\n",
    "            callback_manager=callback_manager,\n",
    "            object_map=object_map,\n",
    "            objects=objects,\n",
    "            verbose=verbose,\n",
    "        )\n",
    "    \n",
    "    def _retrieve(self, query_bundle: QueryBundle) -> List[NodeWithScore]:\n",
    "        query = query_bundle.query_str\n",
    "\n",
    "        tokenized_query = [[word for word in jieba.cut_for_search(query) if word not in self.stop_words]]\n",
    "        \n",
    "        indexes, scores = self.bm25.retrieve(\n",
    "            tokenized_query, k=self.similarity_top_k, show_progress=self._verbose\n",
    "        )\n",
    "\n",
    "        # batched, but only one query\n",
    "        indexes = indexes[0]\n",
    "        scores = scores[0]\n",
    "\n",
    "        nodes: List[NodeWithScore] = []\n",
    "        for idx, score in zip(indexes, scores):\n",
    "            # idx can be an int or a dict of the node\n",
    "            if isinstance(idx, dict):\n",
    "                node = metadata_dict_to_node(idx)\n",
    "            else:\n",
    "                node_dict = self.corpus[int(idx)]\n",
    "                node = metadata_dict_to_node(node_dict)\n",
    "            nodes.append(NodeWithScore(node=node, score=float(score)))\n",
    "\n",
    "        return nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "270d364f-4261-48b0-8991-61219fb41d1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "query_gen_prompt_str = (\n",
    "    \"将下面的句子翻译成英文,直接输出翻译后的结果\"\n",
    "    \"原始句子：{query}\"\n",
    "    \"翻译后的句子：\"\n",
    ")\n",
    "query_gen_prompt = PromptTemplate(query_gen_prompt_str)\n",
    "def translate_en(llm,query_str:str):\n",
    "    fmt_prompt = query_gen_prompt.format(\n",
    "        query = query_str,\n",
    "    )\n",
    "    response = llm.complete(fmt_prompt).text\n",
    "    return response\n",
    "query = \"存储Point类型数据的格式是什么？\"\n",
    "rewrite_query = translate_en(llm,query)\n",
    "print(rewrite_query)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5969921c-f14d-4bc6-93a5-349f92a4d662",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from openai import OpenAI\n",
    "\n",
    "client = OpenAI(\n",
    "    # 若没有配置环境变量，请用百炼API Key将下行替换为：api_key=\"sk-xxx\",\n",
    "    api_key=\"sk-xxx\", \n",
    "    base_url=\"https://dashscope.aliyuncs.com/compatible-mode/v1\",\n",
    ")\n",
    "# query_gen_prompt_str = (\n",
    "#     \"请你对原始问题进行改写，遵循一些修改规则：\\n\"\n",
    "#     \"1.改写后的问题需保留关键信息。\\n\"\n",
    "#     \"2.改写后的问题主谓语需完整。\\n\"\n",
    "#     \"3.对于明显存在两个或以上子问题的句子，请将他们拆成两个完整独立的问题。\\n\"\n",
    "#     \"4.如果该句子中存在tugraph（不区分大小写）都将其从句子中删去。\\n\"\n",
    "#     \"5.对于问题意图不明确的问题，请加上适当的疑问词。\\n\"\n",
    "#     \"6.直接输出改写后的问题，不要做任何解释。\\n\"\n",
    "#     \"原始问题：{query}\"\n",
    "#     \"改写后的问题：\"\n",
    "# )\n",
    "query_gen_prompt_str = (\n",
    "    \"请你对以下用户提出的问题进行改写，遵循以下修改规则：\\n\"\n",
    "    \"1. 改写后的问题需保留原句关键信息和核心意图，避免冗余改动。\\n\"\n",
    "    \"2. 对于意图不明确的问题，请添加一种适当的疑问词，使问题更清晰。\\n\"\n",
    "    \"3. 若句子结构已经简洁明了且主旨明确，则采用最小改动原则。\\n\"\n",
    "    \"4. 对于明显存在两个或以上子问题的句子，将其拆分为两个独立的完整问题。\\n\"\n",
    "    \"5. 句子中的英文词汇不要变动\\n\"\n",
    "    \"6. 直接输出改写后的问题，无需解释。\\n\"\n",
    "    \"原始问题：{query}\\n\"\n",
    "    \"改写后的问题：\"\n",
    ")\n",
    "def query_rewrite(llm,query_str:str):\n",
    "    fmt_prompt = query_gen_prompt_str.format(\n",
    "            query = query\n",
    "        )\n",
    "    completion = client.chat.completions.create(\n",
    "        model=\"qwen2.5-72b-instruct\", # 模型列表：https://help.aliyun.com/zh/model-studio/getting-started/models\n",
    "        messages=[\n",
    "            {'role': 'system', 'content': 'You are a helpful assistant.'},\n",
    "            {'role': 'user', 'content': fmt_prompt}],\n",
    "        temperature = 0.2\n",
    "        )\n",
    "    response = completion.choices[0].message.content\n",
    "    return response\n",
    "query = \"tugraph可以最多创建多少点边和点边上最多创建多少属性？\"\n",
    "response = query_rewrite(llm,query)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3700c054-575a-4fea-8906-a81184384622",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.retrievers import VectorIndexRetriever\n",
    "import copy\n",
    "class EnglishVectorRetriever(VectorIndexRetriever):\n",
    "    def _retrieve(self,query_bundle: QueryBundle,) -> List[NodeWithScore]:\n",
    "        query_bundle_en = copy.copy(query_bundle)\n",
    "        query_str = query_bundle_en.query_str\n",
    "        English_query_str = translate_en(llm,query_str)\n",
    "        query_bundle_en.query_str = English_query_str\n",
    "        if self._vector_store.is_embedding_query:\n",
    "            query_bundle_en.embedding = None\n",
    "            if query_bundle_en.embedding is None and len(query_bundle_en.embedding_strs) > 0:\n",
    "                query_bundle_en.embedding = (\n",
    "                    self._embed_model.get_agg_embedding_from_queries(\n",
    "                        query_bundle_en.embedding_strs\n",
    "                    )\n",
    "                )\n",
    "\n",
    "        return self._get_nodes_with_embeddings(query_bundle_en)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2f79850-5a15-448b-bbee-a8d1224feff2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.retrievers import VectorIndexRetriever\n",
    "import re\n",
    "import copy\n",
    "class MyVectorRetriever(VectorIndexRetriever):\n",
    "    def _retrieve(self,query_bundle: QueryBundle,) -> List[NodeWithScore]:\n",
    "        query_bundle_remove_tugraph_str = re.sub(r'(?i)tugraph', \"\", query_bundle.query_str).strip()\n",
    "        query_bundle_remove_tugraph = copy.copy(query_bundle)\n",
    "        query_bundle_remove_tugraph.query_str = query_bundle_remove_tugraph_str\n",
    "\n",
    "        if self._vector_store.is_embedding_query:\n",
    "            query_bundle_remove_tugraph.embedding = None\n",
    "            if query_bundle_remove_tugraph.embedding is None and len(query_bundle_remove_tugraph.embedding_strs) > 0:\n",
    "                query_bundle_remove_tugraph.embedding = (\n",
    "                    self._embed_model.get_agg_embedding_from_queries(\n",
    "                        query_bundle_remove_tugraph.embedding_strs\n",
    "                    )\n",
    "                )\n",
    "\n",
    "        return self._get_nodes_with_embeddings(query_bundle_remove_tugraph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8614c1b5-26c4-4615-9e60-df9fd09da60a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.retrievers.bm25 import BM25Retriever\n",
    "from llama_index.core.node_parser import SentenceSplitter\n",
    "from llama_index.core.response.notebook_utils import display_source_node\n",
    "\n",
    "from llama_index.core import Document\n",
    "\n",
    "\n",
    "# documents = [Document(text=\"床前明月光\"),\n",
    "#              Document(text=\"疑是地上霜\"),\n",
    "#              Document(text=\"举头望明月\"),\n",
    "#              Document(text=\"低头思故乡\")]\n",
    "\n",
    "# splitter = SentenceSplitter(chunk_size=1024)\n",
    "# nodes = splitter.get_nodes_from_documents(documents)\n",
    "\n",
    "query = \"TuGraph 中使用的两种主要图分析操作是什么？\"\n",
    "## 向量检索\n",
    "vector_retriever = MyVectorRetriever(\n",
    "    index,\n",
    "    similarity_top_k=30\n",
    ")\n",
    " \n",
    "## bm25 关键词检索\n",
    "bm25_retriever = BM25Retriever.from_defaults(\n",
    "    # docstore=index.docstore,\n",
    "    nodes = nodes,\n",
    "    similarity_top_k=3,\n",
    "    # tokenizer=chinese_tokenizer\n",
    ")\n",
    "ChineseBM25_Retriever = ChineseBM25Retriever(nodes=nodes,similarity_top_k=15)\n",
    "EnglishVector_Retriever = EnglishVectorRetriever(index,similarity_top_k=15)\n",
    "\n",
    "\n",
    "# Nodes = EnglishVector_Retriever.retrieve(query)\n",
    "# Nodes = ChineseBM25_Retriever.retrieve(query)\n",
    "Nodes = vector_retriever.retrieve(query)\n",
    "for node in Nodes:\n",
    "    print(node)\n",
    "    # display_source_node(node)\n",
    "# print(vector_retriever.retrieve(query))\n",
    "# print(bm25_retriever.retrieve(query))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3415977-6e40-421c-88dd-a3a3906ce61e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.retrievers import QueryFusionRetriever\n",
    "# class MyQueryFusionRetriever(QueryFusionRetriever):\n",
    "#     def _run_sync_queries(\n",
    "#         self, queries: List[QueryBundle]\n",
    "#     ):\n",
    "#         results = {}\n",
    "#         for query in queries:\n",
    "#             for i, retriever in enumerate(self._retrievers):\n",
    "#                 print(retriever)\n",
    "#                 results[(query.query_str, i)] = retriever.retrieve(query)\n",
    "#                 if i==2:\n",
    "#                     nodes_with_scores = results[(query.query_str, i)]\n",
    "#                     for node in nodes_with_scores:\n",
    "#                         print(f\"Score: {node.score:.2f} - {node.text}...\\n-----\")\n",
    "#         return results\n",
    "# 定义混合retreiver\n",
    "retriever = QueryFusionRetriever(\n",
    "    [vector_retriever, ChineseBM25_Retriever,EnglishVector_Retriever],\n",
    "    retriever_weights=[0.6, 0.3,0.1],\n",
    "    similarity_top_k=40,\n",
    "    num_queries=1,  # set this to 1 to disable query generation\n",
    "    mode=\"reciprocal_rerank\",\n",
    "    use_async=False,\n",
    "    verbose=True,\n",
    ")\n",
    "\n",
    "nodes_with_scores = retriever.retrieve(\n",
    "    \"应该如何写入图数据库中的顶点数据？\"\n",
    ")\n",
    "# print(nodes_with_scores)\n",
    "for node in nodes_with_scores:\n",
    "    # print(f\"Score: {node.score:.2f} - {node.text}...\\n-----\")\n",
    "    print(node.get_content()+'------------')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6690eb9f-b8d5-4cbf-a913-3e2c9142d597",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "val = pd.read_csv(\"./your_project/vals.csv\")\n",
    "# val = val[10:].reset_index(drop=True)\n",
    "# val = df.sample(n=40)\n",
    "# val_drop = df.drop(val.index).reset_index(drop=True)\n",
    "# val = val.reset_index(drop=True)\n",
    "model = SentenceTransformer(\"./your_project/sentence-transformers/paraphrase-MiniLM-L6-v2\")\n",
    "embeddings = model.encode(val['input_field'])\n",
    "def getshots(query):\n",
    "    # val = pd.read_csv(\"./your_project/vals.csv\")\n",
    "    # val = val.drop(index=i)\n",
    "    # val = val.reset_index(drop=True)\n",
    "    embeddings = model.encode(val['input_field'])\n",
    "    query_embed = model.encode(query)\n",
    "    \n",
    "    query_embedding = query_embed.reshape(1, -1)\n",
    "    \n",
    "    # 计算余弦相似度\n",
    "    similarities = cosine_similarity(embeddings, query_embedding).flatten()\n",
    "    \n",
    "    # 按相似度从高到低排序，提取前 4 个索引\n",
    "    top_5_indices = np.argsort(similarities)[-38:][::-1]\n",
    "    # 获取最相似的5个句子 embedding\n",
    "    top_5_embeddings = embeddings[top_5_indices]\n",
    "    # top_5_sentences = val.loc[16][\"input_field\"]\n",
    "    example_shots = \"\"\n",
    "    for index,i in enumerate(top_5_indices):\n",
    "        Q = val.loc[i][\"input_field\"]\n",
    "        A = val.loc[i][\"output_field\"]\n",
    "        S = val.loc[i][\"style\"]\n",
    "        example_shots += \"{index}.问答对示例：\\n\".format(index = index+1)   + \"Q:\" + Q + \"\\n\"  + \"A:\" + A + \"\\n\"   + S + \"\\n\\n\"\n",
    "    qa_prompt_tmpl_str = (\n",
    "        \"We have provided context information below. \\n\"\n",
    "        # \"如果上下文给出了非常明确的答案，那么请优先以上下文的原文为主\\n\"\n",
    "        \"---------------------\\n\"\n",
    "        \"{context_str}\"\n",
    "        \"\\n---------------------\\n\"\n",
    "\"\"\"请参考以下问答对，并注意学习这些回答的风格特点：\\n\n",
    "{example_shots}请根据这些示例，按照如下风格回答接下来的问题：\n",
    "    - 回答应简洁直接，不包含多余的描述。\n",
    "    - 无需重复问题，直接给出答案。\n",
    "    - 保持回答正式且专业。\n",
    "    - 回答中如果出现Tugraph为主语将其省略。\n",
    "    - 回答要避免举例说明。\n",
    "\"\"\"\n",
    "        \"Given this information, please answer the question: {query_str}\\n\"\n",
    "    )\n",
    "    return qa_prompt_tmpl_str.format(context_str=\"{context_str}\",\n",
    "        example_shots=example_shots,\n",
    "        query_str=\"{query_str}\")\n",
    "print(getshots(\"RPC 及 HA 服务中，verbose 参数的设置有几个级别？\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ad20dfe-9b18-4efd-95d7-b35a020cf046",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core import QueryBundle\n",
    "from llama_index.core.query_engine import RetrieverQueryEngine\n",
    "from llama_index.core import PromptTemplate\n",
    "from llama_index.core.postprocessor import SentenceTransformerRerank\n",
    "from llama_index.core.postprocessor import LLMRerank\n",
    "from llama_index.core.indices.utils import default_parse_choice_select_answer_fn\n",
    "import re\n",
    "\n",
    "def custom_parse_choice_select_answer_fn(answer: str, num_choices: int):\n",
    "    matches = re.findall(r\"(Doc: \\d+, Relevance: \\d+)\", answer)\n",
    "    # 按行输出\n",
    "    answer = \"\"\n",
    "    for match in matches:\n",
    "        answer += match+\"\\n\"\n",
    "    _answer = answer\n",
    "    return default_parse_choice_select_answer_fn(_answer, num_choices)\n",
    "\n",
    "rerank = SentenceTransformerRerank(model=\"./your_project/BAAI/bge-reranker-v2-m3\", top_n =5 )\n",
    "# rerank = SentenceTransformerRerank(model=\"/mnt/sda/cl/competition/BDCI/bge-reranker-large\", top_n =5 )\n",
    "# 定义query engine\n",
    "qa_prompt_tmpl_str = (\n",
    "    \"We have provided context information below. \\n\"\n",
    "    \"---------------------\\n\"\n",
    "    \"{context_str}\"\n",
    "    \"\\n---------------------\\n\"\n",
    "    \"\"\"请参考以下问答对，并注意总结这些回答的风格特点：\\n\n",
    "    1. 问答对示例：\\n\n",
    "    Q: RPC 及 HA 服务中，verbose 参数的设置有几个级别？\\n\n",
    "    A: 三个级别（0，1，2）。\\n\n",
    "    （回答风格：简洁明了，直接给出结果，没有多余描述。）\\n\n",
    "    \n",
    "    2. 问答对示例：\\n\n",
    "    Q: 如果成功修改一个用户的描述，应返回什么状态码？\\n\n",
    "    A: 200\\n\n",
    "    （回答风格：技术精准，只给出具体的数值，没有解释。）\\n\n",
    "    \n",
    "    3. 问答对示例：\\n\n",
    "    Q: TuGraph-DB的日志等级如何调整？\\n\n",
    "    A: 单机模式下，调整配置文件 src/server/lgraph_standalone.json，其中 verbose 配置项控制日志等级，verbose 可以设置为 0, 1, 2，对应日志等级可以参考 src/server/lgraph_server.cpp 中 115 行至 128 行。\n",
    "    （回答风格：结构化且明确，提供了操作步骤和具体路径，信息详细但不冗余。）\\n\n",
    "    \n",
    "    4. 问答对示例：\\n\n",
    "    Q: 如果要在 FrontierTraversal 中并行执行遍历，事务的哪种模式必须被选用？\\n\n",
    "    A: 事务必须是只读的。\\n\n",
    "    （回答风格：直接回答，精确描述，没有额外信息。）\\n\n",
    "    \n",
    "    5. 问答对示例：\\n\n",
    "    Q: 在线全量导入 TuGraph 时，如果发生数据包错误，默认行为是什么？\\n\n",
    "    A: 默认行为是在第一个错误包处停止导入。\\n\n",
    "    （回答风格：简单直接，清晰描述默认行为。）\\n\n",
    "    \n",
    "    请根据这些示例，按照如下风格回答接下来的问题：\\n\n",
    "    - 回答应简洁直接，不包含多余的描述。\n",
    "    - 技术术语应准确无误，避免使用模糊的词语。\n",
    "    - 对于涉及参数、配置的回答，提供明确的数值和路径信息。\n",
    "    - 无需重复问题，直接给出答案。\n",
    "    - 保持回答正式且专业。 \"\"\"\n",
    "    \"Given this information, please answer the question: {query_str}\\n\"\n",
    "    \n",
    "    # \"回答要直接简洁，且避免重复回答，回答限制在一两句话之内!\"\n",
    "    # \"回答直接给出结论,以上下文的原文为主。\\n\"\n",
    "    # \"若给出的上下文不足以得出正确答案，请给予恰当的回复，如暂不支持等。\\n\"\n",
    ")\n",
    "\n",
    "query = \"调用算法 `algo.shortestPath` 的实际应用场景是什么？\"\n",
    "qa_prompt_tmpl_str = getshots(query)\n",
    "# print(qa_prompt_tmpl_str)\n",
    "qa_prompt_tmpl = PromptTemplate(qa_prompt_tmpl_str)\n",
    "query_engine = RetrieverQueryEngine.from_args(\n",
    "    retriever,\n",
    "    text_qa_template = qa_prompt_tmpl,\n",
    "    node_postprocessors=[rerank],\n",
    ")\n",
    "# rewrite_query = query_rewrite(llm,query)\n",
    "print(query)\n",
    "response = query_engine.query(query)\n",
    "print(response.response+\"-------------答案结束\")\n",
    "query_bundle = QueryBundle(query)\n",
    "nodes = rerank.postprocess_nodes(retriever.retrieve(query),query_bundle)\n",
    "for node in nodes:\n",
    "    print(node.get_content()+\"------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b792532-035d-4858-a2f4-d4a4ab857976",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "import re\n",
    "# 读取 JSON 文件为 DataFrame\n",
    "file_path = test_path\n",
    "# file_path = '/mnt/sda/cl/competition/BDCI/val.json'\n",
    "# file_path = '/mnt/sda/cl/competition/BDCI/test1.json'\n",
    "# rerank = LLMRerank(llm=llm,top_n=3,parse_choice_select_answer_fn=custom_parse_choice_select_answer_fn)\n",
    "data = []\n",
    "with open(file_path, 'r', encoding='utf-8') as file:\n",
    "    for line in file:\n",
    "        data.append(json.loads(line))\n",
    "\n",
    "# 转换为 DataFrame\n",
    "df = pd.DataFrame(data)\n",
    "query_responses = []\n",
    "# 遍历每个 input_field\n",
    "for i, row in tqdm(df.iterrows(), total=df.shape[0]):\n",
    "    query = row['input_field']\n",
    "    qa_prompt_tmpl_str = getshots(query)\n",
    "    qa_prompt_tmpl = PromptTemplate(qa_prompt_tmpl_str)\n",
    "    query_engine = RetrieverQueryEngine.from_args(\n",
    "        retriever,\n",
    "        text_qa_template = qa_prompt_tmpl,\n",
    "        node_postprocessors=[rerank],\n",
    "    )\n",
    "    rewrite_query = query_rewrite(llm,query)\n",
    "    # print(\"问题: \"+query+\"\\n\")\n",
    "    print(\"改写后:\"+rewrite_query+\"\\n\")\n",
    "    response = query_engine.query(rewrite_query)  # 调用查询引擎\n",
    "    print(response.response)\n",
    "    query_responses.append(response.response)  # 将响应结果添加到列表中\n",
    "# 将查询结果列表添加到 DataFrame 的新列 'query_response'\n",
    "\n",
    "# test\n",
    "df['output_field'] = query_responses\n",
    "# val\n",
    "# df['query_response'] = query_responses\n",
    "\n",
    "# 显示结果\n",
    "df.head(5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99123caa-a852-46da-80d8-8faa8a8980fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "# 1. 选择需要导出的列\n",
    "selected_df = df[['id', 'output_field']]\n",
    "output_dir = \"../output\"\n",
    "output_file = os.path.join(output_dir, \"answer.jsonl\")\n",
    "if not os.path.exists(output_dir):\n",
    "    os.makedirs(output_dir, exist_ok=True)  # exist_ok=True 避免重复创建时报错\n",
    "    print(f\"Directory {output_dir} created.\")\n",
    "# 2. 将 DataFrame 导出为 JSONL 格式的文件\n",
    "selected_df.to_json(output_file, orient='records', lines=True, force_ascii=False)\n",
    "print(\"结果已导出到：\"+output_file)\n",
    "\n",
    "output_dir = \"/output\"\n",
    "output_file = os.path.join(output_dir, \"answer.jsonl\")\n",
    "if not os.path.exists(output_dir):\n",
    "    os.makedirs(output_dir, exist_ok=True)  # exist_ok=True 避免重复创建时报错\n",
    "    print(f\"Directory {output_dir} created.\")\n",
    "# 2. 将 DataFrame 导出为 JSONL 格式的文件\n",
    "selected_df.to_json(output_file, orient='records', lines=True, force_ascii=False)\n",
    "print(\"结果已导出到：\"+output_file)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
